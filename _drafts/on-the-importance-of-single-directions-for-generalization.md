---
layout: "post"
title: "on the importance of single directions for generalization"
date: "2018-04-09 14:35"
comments: true
categories: []
---


### Connection to manifold hypothesis


### Implcation for Neuroscience

The authors shows that importance was unrelated to or possibly negatively related to selectivity. Meaning that the parts of the network that were most important for a particular object to be recognized, were not those that were more active in response to images containing that object. Although only three networks were considered here, we can consider whether this is a general description of representations that support good generalizaion? And if so, would this also apply to biological neural systems? What do we need theoretically to make that jump? Can we describe representations that support generalization _generally_, independent of their material implementation? Sounds like Marr's representation level. That is definitely what attracted me to Marr in the first place, a desire to describe AI systems and BI systems at a common 'level'.

Forgetting that problem for now, let's pretend that we have a means of translating inferences about artificial systems to biological systems. What would it mean for neuroscience (neurophysiology, electrophysiology, neuroimaging) if the importance of a neuron or brain region for performing some task was unrelated to the degree to which it responds selectivity to the task conditions? To some extent, we've arleady had this conversation in neuroscience when we talked about the possibility of grandmother cells (aka gnostic neurons).

Entorhinal cortex has cells that appear to encode abstract properties of physical space. Border cells, gris cells, speed cells, head direction cells.

trade off between the benefits of modularity and the benefit of distributed representation
